import os
import pdfplumber
from bs4 import BeautifulSoup
import pandas as pd

# ðŸ“‚ Folder containing PDFs
pdf_folder = r"C:\Users\Vishal\Downloads\PDF_Files"

# Output files
html_file = "output_with_tables.html"
excel_file = "final_output.xlsx"

html_content = "<html><body>"

all_filtered_results = []  # To collect results from all PDFs

# Loop through all PDF files in the folder
for pdf_name in os.listdir(pdf_folder):
    if pdf_name.lower().endswith(".pdf"):
        pdf_path = os.path.join(pdf_folder, pdf_name)
        print(f" Processing: {pdf_name}")

        with pdfplumber.open(pdf_path) as pdf:
            for i, page in enumerate(pdf.pages, start=1):
                html_content += f"<h2>{pdf_name} - Page {i}</h2>"

                tables = page.extract_tables()
                if tables:
                    for table in tables:
                        html_content += "<table border='1'>"
                        for row in table:
                            html_content += "<tr>"
                            for cell in row:
                                html_content += f"<td>{cell if cell else ''}</td>"
                            html_content += "</tr>"
                        html_content += "</table><br>"

                page_text = page.extract_text()
                if page_text:
                    safe_text = page_text.replace("\n", "<br>")
                    html_content += f"<p>{safe_text}</p>"

# Close HTML content
html_content += "</body></html>"

# Save HTML file
with open(html_file, "w", encoding="utf-8") as f:
    f.write(html_content)

print("âœ… All PDFs converted into structured HTML with tables.")

# Re-open HTML with BeautifulSoup
with open(html_file, "r", encoding="utf-8") as f:
    soup = BeautifulSoup(f, "html.parser")

# Keywords to search
target_keywords = [
    "Polymer (%)",
    "Extractable content (%)",
    "Carbon Black (%)",
    "Ash (%)",
    "HCl Soluble Ash (%)",
    "HCl Insoluble Ash (%)",
    "Polymer Type and Blend ratio"
]

# Extract tables from HTML
all_tables = []
for idx, table in enumerate(soup.find_all("table"), start=1):
    rows = []
    for row in table.find_all("tr"):
        cells = [cell.get_text(strip=True) for cell in row.find_all("td")]
        if cells:
            rows.append(cells)
    df = pd.DataFrame(rows)
    all_tables.append(df)

# Filter based on keywords
for df in all_tables:
    for _, row in df.iterrows():
        for cell in row:
            if any(keyword in str(cell) for keyword in target_keywords):
                all_filtered_results.append(row.tolist())

# Save results to Excel
with pd.ExcelWriter(excel_file, engine="openpyxl") as writer:
    if all_filtered_results:
        df_filtered = pd.DataFrame(all_filtered_results)
        df_filtered.to_excel(writer, sheet_name="Filtered_Data", index=False)

print(f"âœ… Extracted required keywords from all PDFs and saved to {excel_file}")
